Chapter 6: Exercise 7
========================================================

### a
The likelihood for the data is:

$$
\begin{aligned}
    L(\theta \mid \beta)
    &= p(\beta \mid \theta)
    \\
    &= p(\beta_1 \mid \theta)
    \times \cdots
    \times p(\beta_n \mid \theta)
    \\
    &= \prod_{i = 1}^{n}
    p(\beta_i \mid \theta)
    \\
    &= \prod_{i = 1}^{n}
    \frac{
        1
    }{
        \sigma \sqrt{2\pi}
    }
    \exp
    \left(-
        \frac{
            Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
        }{
            2\sigma^2
        }
    \right)
    \\
    &=
    \left(
        \frac{
            1
        }{
            \sigma \sqrt{2\pi}
        }
    \right)^n
    \exp
    \left(
        - \frac{
            1
        }{
            2\sigma^2
        }
        \sum_{i = 1}^{n}
        \left[
            Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
        \right]^2
    \right)
\end{aligned}
$$

### b
The posterior with double exponential (Laplace Distribution) with mean 0 and
common scale parameter $b$, i.e. $p(\beta) = \frac{1}{2b}\exp(- \lvert \beta
\rvert / b)$ is:

$$
    f(\beta \mid X, Y)
    \propto f(Y \mid X, \beta) p(\beta \mid X)
    = f(Y \mid X, \beta) p(\beta)
$$

Substituting our values from (a) and our density function gives us:

$$
\begin{aligned}
    f(Y \mid X, \beta)p(\beta)
    &=
    \left(
        \frac{
            1
        }{
            \sigma \sqrt{2\pi}
        }
    \right)^n
    \exp
    \left(
        - \frac{
            1
        }{
            2\sigma^2
        }
        \sum_{i = 1}^{n}
        \left[
            Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
        \right]^2
    \right)
    \left(
        \frac{
            1
        }{
            2b
        }
        \exp(- \lvert \beta \rvert / b)
    \right)
    \\
    &=
    \left(
        \frac{
            1
        }{
            \sigma \sqrt{2\pi}
        }
    \right)^n
    \left(
        \frac{
            1
        }{
            2b
        }
    \right)
    \exp
    \left(
        - \frac{
            1
        }{
            2\sigma^2
        }
        \sum_{i = 1}^{n}
        \left[
            Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
        \right]^2
        -
        \frac{
            \lvert \beta \rvert
        }{
            b
            }
    \right)
\end{aligned}
$$


### c
Let's simplify it by taking the logarithm of both sides gives:

$$
\begin{aligned}
    \log
    f(Y \mid X, \beta)p(\beta)
    &=
    \log
    \left[
        \left(
            \frac{
                1
            }{
                \sigma \sqrt{2\pi}
            }
        \right)^n
        \left(
            \frac{
                1
            }{
                2b
            }
        \right)
        \exp
        \left(
            - \frac{
                1
            }{
                2\sigma^2
            }
            \sum_{i = 1}^{n}
            \left[
                Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
            \right]^2
            -
            \frac{
                \lvert \beta \rvert
            }{
                b
                }
        \right)
    \right]
    \\
    &=
    \log
    \left[
        \left(
            \frac{
                1
            }{
                \sigma \sqrt{2\pi}
            }
        \right)^n
        \left(
            \frac{
                1
            }{
                2b
            }
        \right)
    \right]
    -
    \left(
        \frac{
            1
        }{
            2\sigma^2
        }
        \sum_{i = 1}^{n}
        \left[
            Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
        \right]^2
        +
        \frac{
            \lvert \beta \rvert
        }{
            b
        }
    \right)
\end{aligned}
$$

We want to maximize the posterior, this means:
$$
\begin{aligned}
    \arg\max_\beta \, f(\beta \mid X, Y)
    &=
    \arg\max_\beta
    \,
    \log
    \left[
        \left(
            \frac{
                1
            }{
                \sigma \sqrt{2\pi}
            }
        \right)^n
        \left(
            \frac{
                1
            }{
                2b
            }
        \right)
    \right]
    -
    \left(
        \frac{
            1
        }{
            2\sigma^2
        }
        \sum_{i = 1}^{n}
        \left[
            Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
        \right]^2
        +
        \frac{
            \lvert \beta \rvert
        }{
            b
            }
    \right)
    \\
\end{aligned}
$$

Since we are taking the difference of two values, the maximum of this value is
the equivalent to taking the difference of the second value in terms of
$\beta$. This results in:

$$
\begin{aligned}
    &=
    \arg\min_\beta
    \,
    \frac{
        1
    }{
        2\sigma^2
    }
    \sum_{i = 1}^{n}
    \left[
        Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
    \right]^2
    +
    \frac{
        \lvert \beta \rvert
    }{
        b
    }
    \\
    &=
    \arg\min_\beta
    \,
    \frac{
        1
    }{
        2\sigma^2
    }
    \sum_{i = 1}^{n}
    \left[
        Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
    \right]^2
    +
    \frac{
        1
    }{
        b
    }
    \sum_{j = 1}^{p} \lvert \beta_j \rvert
    \\
    &=
    \arg\min_\beta
    \,
    \frac{
        1
    }{
        2\sigma^2
    }
    \left(
        \sum_{i = 1}^{n}
        \left[
            Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
        \right]^2
        +
        \frac{
            2\sigma^2
        }{
            b
        }
        \sum_{j = 1}^{p} \lvert \beta_j \rvert
    \right)
\end{aligned}
$$

By letting $\lambda = 2\sigma^2/b$, we can see that we end up with:

$$
\begin{aligned}
    &=
    \arg\min_\beta
    \,
    \sum_{i = 1}^{n}
    \left[
        Y_i - (\beta_0 + \sum_{j = 1}^{p} \beta_j X_{ij})
    \right]^2
    +
    \lambda
    \sum_{j = 1}^{p} \lvert \beta_j \rvert
    \\
    &=
    \arg\min_\beta
    \,
    \text{RSS}
    +
    \lambda
    \sum_{j = 1}^{p} \lvert \beta_j \rvert
\end{aligned}
$$

which we know is the Lasso from equation 6.7 in the book.

### d

### e
